# EMNLP2025-Claim-Verification-Survey

Survey-Paper-List.xlsx	List of 316 papers included in the survey.
case-study1	Annotation results for Case Study #1.
case-study2	Annotation results for Case Study #2.


Research Questions
1. What corpora are available for claim verification and how are they created?
2. What system designs and modeling strategies dominate current CV research?
3. What challenges persist in corpus and system development, and how can they be addressed?

Case Study #1
This study investigates how the number and nature of evidence-bearing sentences (EBSs) affect the difficulty of veracity annotation and verification.
Using three datasets: HealthFC (Vladika et al., 2024), MSVEC (Evans et al., 2023), and WiCE (Kamoi et al., 2023), the analysis measures how many sentences typically support or refute a claim and how consistently human annotators can assign veracity labels.

Case Study #2 â€” Claim Decomposition
This study examines how decomposing complex claims into subclaims influences the accuracy and interpretability of claim verification.
Three representative datasets are analyzed: ClaimDecomp (Chen et al., 2022), FactLens (Mitra et al., 2024), and WiCE (Kamoi et al., 2023), to assess how subclaims are generated, evaluated, and aggregated.




All datasets referenced in this survey are publicly available.
This repository contains metadata, analysis annotations, and summary materials used in the study.
Full annotation data are also available upon request.

If you use this work, please cite our EMNLP 2025 paper.
